# .github/workflows/benchmark.yml
name: Benchmark

on:
  workflow_dispatch:  # Manual triggering with options
    inputs:
      force-run:
        description: 'Force benchmarks to run'
        required: true
        type: boolean
        default: false
  push:
    branches: [ "master" ]
    paths:
      - 'src/**/*.rs'
      # Only run on changes that could affect performance
      - '!src/tests/**'
      - 'benches/**/*.rs'
      # Only consider direct dependency changes
      - 'Cargo.toml'
      - '.github/workflows/benchmark.yml'
  pull_request:
    types: [opened, synchronize, reopened]
    branches: [ "master" ]
    paths:
      - 'src/**/*.rs'
      - '!src/tests/**'
      - 'benches/**/*.rs'
      - 'Cargo.toml'
      - '.github/workflows/benchmark.yml'

# Add concurrency to cancel outdated runs
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  # Add required permissions for commenting on PRs if we want to post results
  pull-requests: write

env:
  CARGO_TERM_COLOR: always
  RUSTFLAGS: "-D warnings"

jobs:
  check-changes:
    runs-on: ubuntu-latest
    outputs:
      should-run: ${{ steps.check.outputs.should-run }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Needed for git diff

      # Get default branch name
      - name: Get default branch name
        id: default-branch
        run: |
          DEFAULT_BRANCH=$(git remote show origin | grep 'HEAD branch' | cut -d' ' -f5)
          echo "name=$DEFAULT_BRANCH" >> $GITHUB_OUTPUT

      - name: Fetch all branches
        run: |
          git fetch --all
          git branch -a

      - id: check
        name: Check if benchmarks are needed
        run: |
          DEFAULT_BRANCH="${{ steps.default-branch.outputs.name }}"
          echo "Default branch is: $DEFAULT_BRANCH"
          echo "Current ref is: ${{ github.ref }}"
          echo "Event type is: ${{ github.event_name }}"

          # Manual trigger check
          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ inputs.force-run }}" == "true" ]]; then
            echo "Manual run requested with force-run=true"
            echo "should-run=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Get list of changed files
          if [[ "${{ github.ref }}" == "refs/heads/$DEFAULT_BRANCH" ]]; then
            echo "On default branch, comparing with previous commit"
            if git rev-parse HEAD^ >/dev/null 2>&1; then
              git diff --name-only HEAD^ HEAD > changes.txt
            else
              git ls-files > changes.txt
            fi
          elif [[ "${{ github.event_name }}" == "pull_request" ]]; then
            echo "On PR, comparing with default branch"
            git diff --name-only origin/$DEFAULT_BRANCH... > changes.txt
          else
            echo "On feature branch, comparing with default branch"
            git diff --name-only origin/$DEFAULT_BRANCH... > changes.txt
          fi

          echo "Changed files:"
          cat changes.txt

          # Check for performance-critical changes
          if grep -q "src/lib.rs\|src/parser.rs\|src/packet.rs" changes.txt; then
            echo "Core functionality changed"
            echo "should-run=true" >> $GITHUB_OUTPUT
          elif grep -q "^benches/" changes.txt; then
            echo "Benchmark code changed"
            echo "should-run=true" >> $GITHUB_OUTPUT
          elif grep -q "Cargo.toml" changes.txt && grep -q "^\[dependencies\]" Cargo.toml; then
            echo "Dependencies changed"
            echo "should-run=true" >> $GITHUB_OUTPUT
          else
            echo "No performance-critical changes"
            echo "should-run=false" >> $GITHUB_OUTPUT
          fi

  benchmark:
    needs: check-changes
    if: needs.check-changes.outputs.should-run == 'true'
    name: Run benchmarks
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install stable toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo
        uses: Swatinem/rust-cache@v2

      # Only run full benchmarks on master or if explicitly requested
      - name: Run quick benchmarks
        if: github.ref != 'refs/heads/master' && github.event_name != 'workflow_dispatch'
        run: |
          # Run with fewer samples for PR checks
          cargo bench --workspace -- --warm-up-time 1 --measurement-time 2 --sample-size 10
          cargo bench --workspace --features="async" -- --warm-up-time 1 --measurement-time 2 --sample-size 10

      - name: Run full benchmarks
        if: github.ref == 'refs/heads/master' || github.event_name == 'workflow_dispatch'
        run: |
          cargo bench --workspace
          cargo bench --workspace --features="async"

      - name: Store benchmark result
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: target/criterion/**/*.json
          retention-days: 30

- name: Comment PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Function to format time in ns to a readable format
            function formatTime(ns) {
              if (ns < 1000) return `${ns.toFixed(2)}ns`;
              const us = ns / 1000;
              if (us < 1000) return `${us.toFixed(2)}¬µs`;
              const ms = us / 1000;
              if (ms < 1000) return `${ms.toFixed(2)}ms`;
              const s = ms / 1000;
              return `${s.toFixed(2)}s`;
            }

            try {
              // Find all benchmark results (using native fs instead of glob)
              function findJsonFiles(dir) {
                let results = [];
                const files = fs.readdirSync(dir);

                for (const file of files) {
                  const path = `${dir}/${file}`;
                  const stat = fs.statSync(path);

                  if (stat.isDirectory()) {
                    results = results.concat(findJsonFiles(path));
                  } else if (file === 'estimates.json') {
                    results.push(path);
                  }
                }

                return results;
              }

              const files = findJsonFiles('target/criterion');

              let comment = '## üèÉ Benchmark Results\n\n';

              // Group results by test type (sync vs async)
              const syncResults = [];
              const asyncResults = [];

              files.forEach(file => {
                const content = JSON.parse(fs.readFileSync(file, 'utf8'));
                // Get benchmark name from path (e.g., "sync/frame_parsing")
                const pathParts = file.split('/');
                const nameIndex = pathParts.findIndex(part => part === 'criterion') + 1;
                const name = pathParts[nameIndex];

                const result = {
                  name,
                  mean: content.mean.point_estimate,
                  stdDev: content.mean.standard_deviation,
                  throughput: content.throughput
                };

                if (name.startsWith('sync')) {
                  syncResults.push(result);
                } else if (name.startsWith('async')) {
                  asyncResults.push(result);
                }
              });

              // Function to format results section
              function formatResultsSection(title, results) {
                if (results.length === 0) return '';

                let section = `### ${title}\n\n`;
                section += '| Benchmark | Mean | Std Dev | Throughput |\n';
                section += '|-----------|------|----------|------------|\n';

                results.forEach(r => {
                  const throughput = r.throughput ?
                    `${r.throughput.per_iteration.point_estimate.toFixed(2)} ${r.throughput.per_iteration.unit}` :
                    'N/A';

                  section += `| ${r.name} | ${formatTime(r.mean)} | ${formatTime(r.stdDev)} | ${throughput} |\n`;
                });

                return section + '\n';
              }

              // Add sync results if any
              if (syncResults.length > 0) {
                comment += formatResultsSection('Synchronous Operations', syncResults);
              }

              // Add async results if any
              if (asyncResults.length > 0) {
                comment += formatResultsSection('Asynchronous Operations', asyncResults);
              }

              // Add notes
              comment += '\n### Notes\n';
              comment += '- Mean: Average execution time\n';
              comment += '- Std Dev: Standard deviation (lower is more consistent)\n';
              comment += '- Throughput: Operations per second (where applicable)\n';

              if (syncResults.length === 0 && asyncResults.length === 0) {
                comment += '\n‚ö†Ô∏è No benchmark results found. This might indicate an issue with the benchmark run.\n';
              }

              // Create or update comment
              const { data: comments } = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              });

              const existingComment = comments.find(comment =>
                comment.user.login === 'github-actions[bot]' &&
                comment.body.includes('üèÉ Benchmark Results')
              );

              if (existingComment) {
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: existingComment.id,
                  body: comment
                });
              } else {
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: comment
                });
              }

            } catch (error) {
              console.error('Error processing benchmark results:', error);
              const errorComment = '## ‚ö†Ô∏è Error Processing Benchmark Results\n\n' +
                'There was an error while processing the benchmark results. ' +
                'Please check the workflow logs for more details.\n\n' +
                '```\n' + error.message + '\n```';

              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: errorComment
              });
            }
